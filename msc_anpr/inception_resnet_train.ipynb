{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pytesseract as pt\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as xet\n",
    "\n",
    "from glob import glob\n",
    "from skimage import io\n",
    "from shutil import copy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_asset_path = \"images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the glob module to retrieve images and annotations\n",
    "path = glob(image_asset_path+\"*.xml\")\n",
    "\n",
    "# Create a dictionary to store the information\n",
    "labels_dict = dict(filepath=[],xmin=[],xmax=[],ymin=[],ymax=[])\n",
    "\n",
    "# Iterate over all the xml files\n",
    "for filename in path:\n",
    "    # Parse the xml file\n",
    "    info = xet.parse(filename)\n",
    "    root = info.getroot()\n",
    "    member_object = root.find('object')\n",
    "    labels_info = member_object.find('bndbox')\n",
    "\n",
    "    # Retrieve the coordinates\n",
    "    xmin = int(labels_info.find('xmin').text)\n",
    "    xmax = int(labels_info.find('xmax').text)\n",
    "    ymin = int(labels_info.find('ymin').text)\n",
    "    ymax = int(labels_info.find('ymax').text)\n",
    "\n",
    "    # Add the coordinates to the dictionary\n",
    "    labels_dict['filepath'].append(filename)\n",
    "    labels_dict['xmin'].append(xmin)\n",
    "    labels_dict['xmax'].append(xmax)\n",
    "    labels_dict['ymin'].append(ymin)\n",
    "    labels_dict['ymax'].append(ymax)\n",
    "\n",
    "# print first 5 rows of the dictionary\n",
    "pd.DataFrame(labels_dict).head()\n",
    "# Create CSV file with labels from XML files\n",
    "df = pd.DataFrame(labels_dict)\n",
    "df.to_csv('labels.csv',index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = df['filepath'][0]\n",
    "def getFilename(filename):\n",
    "    filename_image = xet.parse(filename).getroot().find('filename').text\n",
    "    filepath_image = os.path.join(image_asset_path,filename_image)\n",
    "    return filepath_image\n",
    "getFilename(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = list(df['filepath'].apply(getFilename))\n",
    "image_path[:10]#random check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = image_path[0] #path of our image N1.jpeg\n",
    "img = io.imread(file_path) #Read the image\n",
    "fig = px.imshow(img)\n",
    "fig.update_layout(width=1000, height=1000, margin=dict(l=10, r=10, b=10, t=10),xaxis_title='Figure 8 - N2.jpeg with bounding box')\n",
    "fig.add_shape(type='rect',x0=1093, x1=1396, y0=645, y1=727, xref='x', yref='y',line_color='cyan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert each and every image into an array using OpenCV and resize the image into 224 x 224 which is the standard compatible size of the pre-trained transfer learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Targeting all our values in array selecting all columns\n",
    "labels = df.iloc[:,1:].values\n",
    "data = []\n",
    "output = []\n",
    "for ind in range(len(image_path)):\n",
    "    image = image_path[ind]\n",
    "    img_arr = cv2.imread(image)\n",
    "    h,w,d = img_arr.shape\n",
    "    # Prepprocesing\n",
    "    load_image = load_img(image,target_size=(224,224))\n",
    "    load_image_arr = img_to_array(load_image)\n",
    "    norm_load_image_arr = load_image_arr/255.0 # Normalization\n",
    "    # Normalization to labels\n",
    "    xmin,xmax,ymin,ymax = labels[ind]\n",
    "    nxmin,nxmax = xmin/w,xmax/w\n",
    "    nymin,nymax = ymin/h,ymax/h\n",
    "    label_norm = (nxmin,nxmax,nymin,nymax) # Normalized output\n",
    "    # Append\n",
    "    data.append(norm_load_image_arr)\n",
    "    output.append(label_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to array\n",
    "X = np.array(data,dtype=np.float32)\n",
    "y = np.array(output,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset into training and testing sets using sklearn.model_selection.train_test_split() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,y,train_size=0.8,random_state=0)\n",
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p id=\"part14\"></p>\n",
    "\n",
    "# <span style=\"font-family: Arials; font-size: 20px; font-style: normal; font-weight: bold; letter-spacing: 3px; text-align: center; line-height:1.0\">4. DEEP LEARNING FOR OBJECT DETECTION </span>\n",
    "\n",
    "<p id=\"part15\"></p>\n",
    "\n",
    "# <span style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; text-align: center; line-height:1.0\">4.1 INCEPTION-RESNET-V2 MODEL BUILDING</span>\n",
    "\n",
    "Inception-ResNet-v2 is a convolutional neural network that is trained on more than a million images from the ImageNet database. The network is 164 layers deep and can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals. As a result, the network has learned rich feature representations for a wide range of images. The Inception-ResNet-v2 was used for the classification task. The architecture of the network is shown in Figure 9 . Inception-Resnet-v2 is formulated based on a combination of the Inception structure and the Residual connection. In the Inception-Resnet block multiple sized convolutional filters are combined by residual connections. The usage of reyfual connections not only avoids the degradation problm caused by deep structures but also reduces the training time.\n",
    "\n",
    "<img src= \"https://github.com/Asikpalysik/Automatic-License-Plate-Detection/blob/main/Presentation/Notebook7.png?raw=true\" width=\"50%\" align=\"center\"  hspace=\"5%\" vspace=\"5%\"/>\n",
    "\n",
    "We are ready to train a deep learning model for object detection. Here we will use the Inception-ResNet-v2 model with pre-trained weights and train this to our data. We are already import necessary libraries from TensorFlow previously, lets continue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_resnet = InceptionResNetV2(weights=\"imagenet\",include_top=False, input_tensor=Input(shape=(224,224,3)))\n",
    "# ---------------------\n",
    "headmodel = inception_resnet.output\n",
    "headmodel = Flatten()(headmodel)\n",
    "headmodel = Dense(500,activation=\"relu\")(headmodel)\n",
    "headmodel = Dense(250,activation=\"relu\")(headmodel)\n",
    "headmodel = Dense(4,activation='sigmoid')(headmodel)\n",
    "\n",
    "\n",
    "# ---------- model\n",
    "model = Model(inputs=inception_resnet.input,outputs=headmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compile the model and  have a look at our summary. Don't de surprise summary will be a bit massiv. The summary is textual and includes information about: The layers and their order in the model. The output shape of each layer. The number of parameters (weights) in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complie model\n",
    "model.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train using the resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_output_dir = os.path.join('output')\n",
    "tfb_logs_dir = os.path.join(root_output_dir + '/object_detection_logs')\n",
    "model_dir = os.path.join(root_output_dir + '/models/object_detection_model.h5')\n",
    "\n",
    "tfb = TensorBoard(tfb_logs_dir)\n",
    "history = model.fit(x=x_train,y=y_train,batch_size=10,epochs=140,\n",
    "                    validation_data=(x_test,y_test),callbacks=[tfb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p id=\"part17\"></p>\n",
    "\n",
    "# <span style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; text-align: center; line-height:1.0\">4.3 TENSORBOARD</span>\n",
    "\n",
    "Lest have a look at on scalars on TensorBoard. In order to do it we will need to run simple command with right path for our \"object detection\". After we will see output with hosted link open it with Chrome. I was using VSCode for this project and for me it was way easy to run TensorBoard overview results, but in Kaggle it a bit more complicated and could  be disscused in other topic. For now i will show one screenshot of result which we have. We can see on scalars *Figure 12* how is our model preform. Our train and validation set don’t have over fitting behavior and our loss with respect of epochs is less.\n",
    "\n",
    "You can simple type <code>!tensorboard --logdir=\"./object_detection\"</code> it will generate link with text, click on link and here we go. <code>Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all TensorBoard 2.6.0 at http://localhost:6006/ (Press CTRL+C to quit)</code>\n",
    "\n",
    "<img src= \"https://github.com/Asikpalysik/Automatic-License-Plate-Detection/blob/main/Presentation/Notebook8.png?raw=true\" width=\"80%\" align=\"center\" hspace=\"5%\" vspace=\"5%\"/>\n",
    "\n",
    "<p id=\"part18\"></p>\n",
    "\n",
    "# <span style=\"font-family: Arials; font-size: 20px; font-style: normal; font-weight: bold; letter-spacing: 3px; text-align: center; line-height:1.0\">5. PIPELINE OBJECT DETECTION MODEL</span>\n",
    "\n",
    "<p id=\"part19\"></p>\n",
    "\n",
    "# <span style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; text-align: center; line-height:1.0\">5.1 MAKE PREDICTIONS</span>\n",
    "\n",
    "This is the final step in object detection. In this step, we will put it all together and get the prediction for a given image. First, I would like to try with one of my test pictures of car. Let load our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 15312), started 0:00:28 ago. (Use '!kill 15312' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4aa9bf891d113eeb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4aa9bf891d113eeb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=\"./output/object_detection_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
